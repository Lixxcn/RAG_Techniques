{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/reranking_with_llamaindex.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG 系统中的重排方法\n",
    "\n",
    "## 概述\n",
    "重排是检索增强生成（RAG）系统中至关重要的一步，旨在提高检索文档的相关性和质量。它涉及重新评估和重新排序最初检索到的文档，以确保最相关的信息在后续处理或呈现中得到优先处理。\n",
    "\n",
    "## 动机\n",
    "在 RAG 系统中进行重排的主要动机是克服初始检索方法的局限性，这些方法通常依赖于更简单的相似性度量。重排允许进行更复杂的关联性评估，考虑到传统检索技术可能忽略的查询和文档之间的细微关系。此过程旨在通过确保在生成阶段使用最相关的信息来增强 RAG 系统的整体性能。\n",
    "\n",
    "## 关键组件\n",
    "重排系统通常包括以下组件：\n",
    "\n",
    "1. 初始检索器：通常是使用基于嵌入的相似性搜索的向量存储。\n",
    "2. 重排模型：可以是：\n",
    "   - 用于评分相关性的大型语言模型（LLM）\n",
    "   - 专门为相关性评估训练的交叉编码器模型\n",
    "3. 评分机制：一种为文档分配相关性分数的方法\n",
    "4. 排序和选择逻辑：根据新分数重新排序文档\n",
    "\n",
    "## 方法详情\n",
    "重排过程通常遵循以下步骤：\n",
    "\n",
    "1. 初始检索：获取一组潜在相关的文档。\n",
    "2. 配对创建：为每个检索到的文档创建查询-文档对。\n",
    "3. 评分：\n",
    "   - LLM 方法：使用提示要求 LLM 评估文档相关性。\n",
    "   - 交叉编码器方法：将查询-文档对直接输入模型。\n",
    "4. 分数解释：解析和规范化相关性分数。\n",
    "5. 重新排序：根据其新的相关性分数对文档进行排序。\n",
    "6. 选择：从重新排序的列表中选择前 K 个文档。\n",
    "\n",
    "## 此方法的优点\n",
    "重排具有以下几个优点：\n",
    "\n",
    "1. 提高相关性：通过使用更复杂的模型，重排可以捕捉到细微的相关性因素。\n",
    "2. 灵活性：可以根据具体需求和资源应用不同的重排方法。\n",
    "3. 增强上下文质量：向 RAG 系统提供更相关的文档可以提高生成响应的质量。\n",
    "4. 减少噪音：重排有助于过滤掉不太相关的信息，专注于最相关的内容。\n",
    "\n",
    "## 结论\n",
    "重排是 RAG 系统中一种强大的技术，可显著提高检索信息的质量。无论是使用基于 LLM 的评分还是专门的交叉编码器模型，重排都可以对文档相关性进行更细致、更准确的评估。这种提高的相关性直接转化为下游任务更好的性能，使重排成为高级 RAG 实现中必不可少的组件。\n",
    "\n",
    "在基于 LLM 和交叉编码器的重排方法之间进行选择，取决于所需准确性、可用计算资源和特定应用需求等因素。两种方法都比基本检索方法有实质性改进，并有助于 RAG 系统的整体有效性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "\n",
    "<img src=\"../images/reranking-visualization.svg\" alt=\"rerank llm\" style=\"width:100%; height:auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "\n",
    "<img src=\"../images/reranking_comparison.svg\" alt=\"rerank llm\" style=\"width:100%; height:auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 包安装和导入\n",
    "\n",
    "下面的单元格安装了运行此笔记本所需的所有必要软件包。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install faiss-cpu llama-index python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from typing import List\n",
    "from llama_index.core import Document\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.readers import SimpleDirectoryReader\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank, LLMRerank\n",
    "from llama_index.core import QueryBundle\n",
    "import faiss\n",
    "\n",
    "\n",
    "# Original path append replaced for Colab compatibility\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Llamaindex global settings for llm and embeddings\n",
    "EMBED_DIMENSION=512\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\", dimensions=EMBED_DIMENSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required data files\n",
    "import os\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Download the PDF document used in this notebook\n",
    "!wget -O data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/\"\n",
    "reader = SimpleDirectoryReader(input_dir=path, required_exts=['.pdf'])\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建向量存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FaisVectorStore to store embeddings\n",
    "fais_index = faiss.IndexFlatL2(EMBED_DIMENSION)\n",
    "vector_store = FaissVectorStore(faiss_index=fais_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 摄取管道"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pipeline = IngestionPipeline(\n",
    "    transformations=[SentenceSplitter()],\n",
    "    vector_store=vector_store,\n",
    "    documents=documents\n",
    ")\n",
    "\n",
    "nodes = base_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: LLM based reranking the retrieved documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "\n",
    "<img src=\"../images/rerank_llm.svg\" alt=\"rerank llm\" style=\"width:40%; height:auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector index from base nodes\n",
    "index = VectorStoreIndex(nodes)\n",
    "\n",
    "query_engine_w_llm_rerank = index.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    node_postprocessors=[\n",
    "        LLMRerank(\n",
    "            top_n=5\n",
    "        )\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = query_engine_w_llm_rerank.query(\"What are the impacts of climate change on biodiversity?\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example that demonstrates why we should use reranking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of Retrieval Techniques\n",
      "==================================\n",
      "Query: what is the capital of france?\n",
      "\n",
      "Baseline Retrieval Result:\n",
      "\n",
      "Document 1:\n",
      "The capital of France is great.\n",
      "\n",
      "Document 2:\n",
      "The capital of France is huge.\n",
      "\n",
      "Advanced Retrieval Result:\n",
      "\n",
      "Document 1:\n",
      "Have you ever visited Paris? It is a beautiful city where you can eat delicious food and see the Eiffel Tower. I really enjoyed all the cities in france, but its capital with the Eiffel Tower is my favorite city.\n",
      "\n",
      "Document 2:\n",
      "I really enjoyed my trip to Paris, France. The city is beautiful and the food is delicious. I would love to visit again. Such a great capital city.\n"
     ]
    }
   ],
   "source": [
    "chunks = [\n",
    "    \"The capital of France is great.\",\n",
    "    \"The capital of France is huge.\",\n",
    "    \"The capital of France is beautiful.\",\n",
    "    \"\"\"Have you ever visited Paris? It is a beautiful city where you can eat delicious food and see the Eiffel Tower. I really enjoyed all the cities in france, but its capital with the Eiffel Tower is my favorite city.\"\"\", \n",
    "    \"I really enjoyed my trip to Paris, France. The city is beautiful and the food is delicious. I would love to visit again. Such a great capital city.\"\n",
    "]\n",
    "docs = [Document(page_content=sentence) for sentence in chunks]\n",
    "\n",
    "\n",
    "def compare_rag_techniques(query: str, docs: List[Document] = docs) -> None:\n",
    "    docs = [Document(text=sentence) for sentence in chunks]\n",
    "    index = VectorStoreIndex.from_documents(docs)\n",
    "    \n",
    "    \n",
    "    print(\"Comparison of Retrieval Techniques\")\n",
    "    print(\"==================================\")\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    \n",
    "    print(\"Baseline Retrieval Result:\")\n",
    "    baseline_docs = index.as_retriever(similarity_top_k=5).retrieve(query)\n",
    "    for i, doc in enumerate(baseline_docs[:2]): # Get only the first two retrieved docs\n",
    "        print(f\"\\nDocument {i+1}:\")\n",
    "        print(doc.text)\n",
    "\n",
    "    print(\"\\nAdvanced Retrieval Result:\")\n",
    "    reranker = LLMRerank(\n",
    "        top_n=2,\n",
    "    )\n",
    "    advanced_docs = reranker.postprocess_nodes(\n",
    "            baseline_docs, \n",
    "            QueryBundle(query)\n",
    "        )\n",
    "    for i, doc in enumerate(advanced_docs):\n",
    "        print(f\"\\nDocument {i+1}:\")\n",
    "        print(doc.text)\n",
    "\n",
    "\n",
    "query = \"what is the capital of france?\"\n",
    "compare_rag_techniques(query, docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Cross Encoder models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "\n",
    "<img src=\"../images/rerank_cross_encoder.svg\" alt=\"rerank cross encoder\" style=\"width:40%; height:auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LlamaIndex has builtin support for [SBERT](https://www.sbert.net/index.html) models that can be used directly as node postprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_w_cross_encoder = index.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    node_postprocessors=[\n",
    "        SentenceTransformerRerank(\n",
    "            model='cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
    "            top_n=5\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "resp = query_engine_w_cross_encoder.query(\"What are the impacts of climate change on biodiversity?\")\n",
    "print(resp)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
