{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/document_augmentation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 通过问题生成增强文档检索的文档增强\n",
    "\n",
    "## 概述\n",
    "\n",
    "此实现演示了一种文本增强技术，该技术利用额外的问题生成来改进向量数据库中的文档检索。通过生成并合并与每个文本片段相关的各种问题，系统增强了标准检索过程，从而增加了找到可用作生成式问答上下文的相关文档的可能性。\n",
    "\n",
    "## 动机\n",
    "\n",
    "通过用相关问题丰富文本片段，我们旨在显著提高识别文档中最相关部分的准确性，这些部分包含用户查询的答案。\n",
    "\n",
    "## 先决条件\n",
    "\n",
    "此方法利用 OpenAI 的语言模型和嵌入。您需要一个 OpenAI API 密钥才能使用此实现。请确保已安装所需的 Python 包：\n",
    "\n",
    "```\n",
    "pip install langchain openai faiss-cpu PyPDF2 pydantic\n",
    "```\n",
    "\n",
    "## 关键组件\n",
    "\n",
    "1. **PDF 处理和文本分块**：处理 PDF 文档并将其划分为可管理的文本片段。\n",
    "2. **问题增强**：使用 OpenAI 的语言模型在文档和片段级别生成相关问题。\n",
    "3. **向量存储创建**：使用 OpenAI 的嵌入模型计算文档的嵌入，并创建一个 FAISS 向量存储。\n",
    "4. **检索和答案生成**：使用 FAISS 找到最相关的文档，并根据提供的上下文生成答案。\n",
    "\n",
    "## 方法详情\n",
    "\n",
    "### 文档预处理\n",
    "\n",
    "1. 使用 LangChain 的 PyPDFLoader 将 PDF 转换为字符串。\n",
    "2. 将文本拆分为重叠的文本文档（text_document）以用于构建上下文，然后将每个文档拆分为重叠的文本片段（text_fragment）以用于检索和语义搜索。\n",
    "\n",
    "### 文档增强\n",
    "\n",
    "1. 使用 OpenAI 的语言模型在文档或文本片段级别生成问题。\n",
    "2. 使用 QUESTIONS_PER_DOCUMENT 常量配置要生成的问题数量。\n",
    "\n",
    "### 向量存储创建\n",
    "\n",
    "1. 使用 OpenAIEmbeddings 类计算文档嵌入。\n",
    "2. 从这些嵌入创建一个 FAISS 向量存储。\n",
    "\n",
    "### 检索和生成\n",
    "\n",
    "1. 根据给定的查询从 FAISS 存储中检索最相关的文档。\n",
    "2. 使用检索到的文档作为上下文，使用 OpenAI 的语言模型生成答案。\n",
    "\n",
    "## 此方法的优点\n",
    "\n",
    "1. **增强的检索过程**：增加了为给定查询找到最相关的 FAISS 文档的概率。\n",
    "2. **灵活的上下文调整**：允许轻松调整文本文档和片段的上下文窗口大小。\n",
    "3. **高质量的语言理解**：利用 OpenAI 强大的语言模型进行问题生成和答案生成。\n",
    "\n",
    "## 实现细节\n",
    "\n",
    "- `OpenAIEmbeddingsWrapper` 类为嵌入生成提供了一致的接口。\n",
    "- `generate_questions` 函数使用 OpenAI 的聊天模型从文本中创建相关问题。\n",
    "- `process_documents` 函数处理文档拆分、问题生成和向量存储创建的核心逻辑。\n",
    "- 主执行演示了加载 PDF、处理其内容并执行示例查询。\n",
    "\n",
    "## 结论\n",
    "\n",
    "该技术提供了一种提高基于向量的文档搜索系统中信息检索质量的方法。通过生成类似于用户查询的其他问题并利用 OpenAI 的高级语言模型，它可能在后续任务（如问答）中带来更好的理解和更准确的响应。\n",
    "\n",
    "## 关于 API 使用的说明\n",
    "\n",
    "请注意，此实现使用 OpenAI 的 API，可能会根据使用情况产生费用。请确保监控您的 API 使用情况并在您的 OpenAI 帐户设置中设置适当的限制。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 包安装和导入\n",
    "\n",
    "下面的单元格安装了运行此笔记本所需的所有必要包。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装所需的包\n",
    "!pip install faiss-cpu langchain langchain-openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 克隆存储库以访问辅助函数和评估模块\n",
    "!git clone https://github.com/NirDiamant/RAG_TECHNIQUES.git\n",
    "import sys\n",
    "sys.path.append('RAG_TECHNIQUES')\n",
    "# 如果您需要使用最新数据运行\n",
    "# !cp -r RAG_TECHNIQUES/data ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from enum import Enum\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "\n",
    "# 为 Colab 兼容性替换了原始路径附加\n",
    "\n",
    "from helper_functions import *\n",
    "\n",
    "\n",
    "class QuestionGeneration(Enum):\n",
    "    \"\"\"\n",
    "    用于指定文档处理问题生成级别的枚举类。\n",
    "\n",
    "    属性：\n",
    "        DOCUMENT_LEVEL (int): 表示在整个文档级别生成问题。\n",
    "        FRAGMENT_LEVEL (int): 表示在单个文本片段级别生成问题。\n",
    "    \"\"\"\n",
    "    DOCUMENT_LEVEL = 1\n",
    "    FRAGMENT_LEVEL = 2\n",
    "\n",
    "#取决于模型，对于 Mitral 7B，最大可以是 8000，对于 Llama 3.1 8B，可以是 128k\n",
    "DOCUMENT_MAX_TOKENS = 4000\n",
    "DOCUMENT_OVERLAP_TOKENS = 100\n",
    "\n",
    "#在较短文本上计算嵌入和文本相似度\n",
    "FRAGMENT_MAX_TOKENS = 128\n",
    "FRAGMENT_OVERLAP_TOKENS = 16\n",
    "\n",
    "#在文档或片段级别生成的问题\n",
    "QUESTION_GENERATION = QuestionGeneration.DOCUMENT_LEVEL\n",
    "#将为特定文档或片段生成多少个问题\n",
    "QUESTIONS_PER_DOCUMENT = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义此管道使用的类和函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionList(BaseModel):\n",
    "    question_list: List[str] = Field(..., title=\"为文档或片段生成的问题列表\")\n",
    "\n",
    "\n",
    "class OpenAIEmbeddingsWrapper(OpenAIEmbeddings):\n",
    "    \"\"\"\n",
    "    OpenAI 嵌入的包装类，提供与原始 OllamaEmbeddings 类似的接口。\n",
    "    \"\"\"\n",
    "    \n",
    "    def __call__(self, query: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        允许将实例用作可调用对象来为查询生成嵌入。\n",
    "\n",
    "        参数：\n",
    "            query (str): 要嵌入的查询字符串。\n",
    "\n",
    "        返回：\n",
    "            List[float]: 查询的嵌入，为浮点数列表。\n",
    "        \"\"\"\n",
    "        return self.embed_query(query)\n",
    "\n",
    "def clean_and_filter_questions(questions: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    清理和过滤问题列表。\n",
    "\n",
    "    参数：\n",
    "        questions (List[str]): 要清理和过滤的问题列表。\n",
    "\n",
    "    返回：\n",
    "        List[str]: 以问号结尾的已清理和过滤的问题列表。\n",
    "    \"\"\"\n",
    "    cleaned_questions = []\n",
    "    for question in questions:\n",
    "        cleaned_question = re.sub(r'^\\d+\\.\\s*', '', question.strip())\n",
    "        if cleaned_question.endswith('?'):\n",
    "            cleaned_questions.append(cleaned_question)\n",
    "    return cleaned_questions\n",
    "\n",
    "def generate_questions(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    使用 OpenAI 根据提供的文本生成问题列表。\n",
    "\n",
    "    参数：\n",
    "        text (str): 用于生成问题的上下文数据。\n",
    "\n",
    "    返回：\n",
    "        List[str]: 唯一、已过滤的问题列表。\n",
    "    \"\"\"\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"context\", \"num_questions\"],\n",
    "        template=\"Using the context data: {context}\\n\\nGenerate a list of at least {num_questions} \"\n",
    "                 \"possible questions that can be asked about this context. Ensure the questions are \"\n",
    "                 \"directly answerable within the context and do not include any answers or headers. \"\n",
    "                 \"Separate the questions with a new line character.\"\n",
    "    )\n",
    "    chain = prompt | llm.with_structured_output(QuestionList)\n",
    "    input_data = {\"context\": text, \"num_questions\": QUESTIONS_PER_DOCUMENT}\n",
    "    result = chain.invoke(input_data)\n",
    "    \n",
    "    # 从 QuestionList 对象中提取问题列表\n",
    "    questions = result.question_list\n",
    "    \n",
    "    filtered_questions = clean_and_filter_questions(questions)\n",
    "    return list(set(filtered_questions))\n",
    "\n",
    "def generate_answer(content: str, question: str) -> str:\n",
    "    \"\"\"\n",
    "    使用 OpenAI 根据提供的上下文为给定问题生成答案。\n",
    "\n",
    "    参数：\n",
    "        content (str): 用于生成答案的上下文数据。\n",
    "        question (str): 为其生成答案的问题。\n",
    "\n",
    "    返回：\n",
    "        str: 基于所提供上下文的对问题的精确回答。\n",
    "    \"\"\"\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\",temperature=0)\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        template=\"Using the context data: {context}\\n\\nProvide a brief and precise answer to the question: {question}\"\n",
    "    )\n",
    "    chain =  prompt | llm\n",
    "    input_data = {\"context\": content, \"question\": question}\n",
    "    return chain.invoke(input_data)\n",
    "\n",
    "def split_document(document: str, chunk_size: int, chunk_overlap: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    将文档拆分为更小的文本块。\n",
    "\n",
    "    参数：\n",
    "        document (str): 要拆分的文档的文本。\n",
    "        chunk_size (int): 每个块的大小（以令牌数为单位）。\n",
    "        chunk_overlap (int): 连续块之间的重叠令牌数。\n",
    "\n",
    "    返回：\n",
    "        List[str]: 文本块列表，其中每个块是文档内容的字符串。\n",
    "    \"\"\"\n",
    "    tokens = re.findall(r'\\b\\w+\\b', document)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), chunk_size - chunk_overlap):\n",
    "        chunk_tokens = tokens[i:i + chunk_size]\n",
    "        chunks.append(chunk_tokens)\n",
    "        if i + chunk_size >= len(tokens):\n",
    "            break\n",
    "    return [\" \".join(chunk) for chunk in chunks]\n",
    "\n",
    "def print_document(comment: str, document: Any) -> None:\n",
    "    \"\"\"\n",
    "    打印注释，后跟文档内容。\n",
    "\n",
    "    参数：\n",
    "        comment (str): 在文档详细信息之前打印的注释或描述。\n",
    "        document (Any): 要打印其内容的文档。\n",
    "\n",
    "    返回：\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(f'{comment} (type: {document.metadata[\"type\"]}, index: {document.metadata[\"index\"]}): {document.page_content}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示例用法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化 OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddingsWrapper()\n",
    "\n",
    "# 示例文档\n",
    "example_text = \"This is an example document. It contains information about various topics.\"\n",
    "\n",
    "# 生成问题\n",
    "questions = generate_questions(example_text)\n",
    "print(\"Generated Questions:\")\n",
    "for q in questions:\n",
    "    print(f\"- {q}\")\n",
    "\n",
    "# 生成答案\n",
    "sample_question = questions[0] if questions else \"What is this document about?\"\n",
    "answer = generate_answer(example_text, sample_question)\n",
    "print(f\"\\nQuestion: {sample_question}\")\n",
    "print(f\"Answer: {answer}\")\n",
    "\n",
    "# 拆分文档\n",
    "chunks = split_document(example_text, chunk_size=10, chunk_overlap=2)\n",
    "print(\"\\nDocument Chunks:\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i + 1}: {chunk}\")\n",
    "\n",
    "# 使用 OpenAIEmbeddings 的示例\n",
    "doc_embedding = embeddings.embed_documents([example_text])\n",
    "query_embedding = embeddings.embed_query(\"What is the main topic?\")\n",
    "print(\"\\nDocument Embedding (first 5 elements):\", doc_embedding[0][:5])\n",
    "print(\"Query Embedding (first 5 elements):\", query_embedding[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主管道"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents(content: str, embedding_model: OpenAIEmbeddings):\n",
    "    \"\"\"\n",
    "    处理文档内容，将其拆分为片段，生成问题，\n",
    "    创建 FAISS 向量存储，并返回一个检索器。\n",
    "\n",
    "    参数：\n",
    "        content (str): 要处理的文档内容。\n",
    "        embedding_model (OpenAIEmbeddings): 用于向量化的嵌入模型。\n",
    "\n",
    "    返回：\n",
    "        VectorStoreRetriever: 一个用于检索最相关 FAISS 文档的检索器。\n",
    "    \"\"\"\n",
    "    # 将整个文本内容拆分为文本文档\n",
    "    text_documents = split_document(content, DOCUMENT_MAX_TOKENS, DOCUMENT_OVERLAP_TOKENS)\n",
    "    print(f'Text content split into: {len(text_documents)} documents')\n",
    "\n",
    "    documents = []\n",
    "    counter = 0\n",
    "    for i, text_document in enumerate(text_documents):\n",
    "        text_fragments = split_document(text_document, FRAGMENT_MAX_TOKENS, FRAGMENT_OVERLAP_TOKENS)\n",
    "        print(f'Text document {i} - split into: {len(text_fragments)} fragments')\n",
    "        \n",
    "        for j, text_fragment in enumerate(text_fragments):\n",
    "            documents.append(Document(\n",
    "                page_content=text_fragment,\n",
    "                metadata={\"type\": \"ORIGINAL\", \"index\": counter, \"text\": text_document}\n",
    "            ))\n",
    "            counter += 1\n",
    "            \n",
    "            if QUESTION_GENERATION == QuestionGeneration.FRAGMENT_LEVEL:\n",
    "                questions = generate_questions(text_fragment)\n",
    "                documents.extend([\n",
    "                    Document(page_content=question, metadata={\"type\": \"AUGMENTED\", \"index\": counter + idx, \"text\": text_document})\n",
    "                    for idx, question in enumerate(questions)\n",
    "                ])\n",
    "                counter += len(questions)\n",
    "                print(f'Text document {i} Text fragment {j} - generated: {len(questions)} questions')\n",
    "        \n",
    "        if QUESTION_GENERATION == QuestionGeneration.DOCUMENT_LEVEL:\n",
    "            questions = generate_questions(text_document)\n",
    "            documents.extend([\n",
    "                Document(page_content=question, metadata={\"type\": \"AUGMENTED\", \"index\": counter + idx, \"text\": text_document})\n",
    "                for idx, question in enumerate(questions)\n",
    "            ])\n",
    "            counter += len(questions)\n",
    "            print(f'Text document {i} - generated: {len(questions)} questions')\n",
    "\n",
    "    for document in documents:\n",
    "        print_document(\"Dataset\", document)\n",
    "\n",
    "    print(f'Creating store, calculating embeddings for {len(documents)} FAISS documents')\n",
    "    vectorstore = FAISS.from_documents(documents, embedding_model)\n",
    "\n",
    "    print(\"Creating retriever returning the most relevant FAISS document\")\n",
    "    return vectorstore.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载所需的数据文件\n",
    "import os\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# 下载本笔记本中使用的 PDF 文档\n",
    "!wget -O data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n",
    "!wget -O data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 将示例 PDF 文档加载到字符串变量中\n",
    "path = \"data/Understanding_Climate_Change.pdf\"\n",
    "content = read_pdf_to_string(path)\n",
    "\n",
    "# 实例化将由 FAISS 使用的 OpenAI Embeddings 类\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "# 处理文档并创建检索器\n",
    "document_query_retriever = process_documents(content, embedding_model)\n",
    "\n",
    "# 检索器使用示例\n",
    "query = \"What is climate change?\"\n",
    "retrieved_docs = document_query_retriever.get_relevant_documents(query)\n",
    "print(f\"\\nQuery: {query}\")\n",
    "print(f\"Retrieved document: {retrieved_docs[0].page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在存储中查找最相关的 FAISS 文档。在大多数情况下，这将是一个增强的问题，而不是原始文本文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How do freshwater ecosystems change due to alterations in climatic factors?\"\n",
    "print (f'Question:{os.linesep}{query}{os.linesep}')\n",
    "retrieved_documents = document_query_retriever.invoke(query)\n",
    "\n",
    "for doc in retrieved_documents:\n",
    "    print_document(\"Relevant fragment retrieved\", doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查找父文本文档并将其用作生成模型生成问题答案的上下文。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = doc.metadata['text']\n",
    "print (f'{os.linesep}Context:{os.linesep}{context}')\n",
    "answer = generate_answer(context, query)\n",
    "print(f'{os.linesep}Answer:{os.linesep}{answer}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
