#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAGè°ƒè¯•å·¥å…·é›†

è¿™ä¸ªæ¨¡å—æä¾›äº†ä¸€å¥—å®Œæ•´çš„RAGç³»ç»Ÿè°ƒè¯•å·¥å…·ï¼Œå¸®åŠ©å¼€å‘è€…æ·±å…¥äº†è§£RAGæµæ°´çº¿ä¸­æ¯ä¸ªç¯èŠ‚çš„å·¥ä½œæƒ…å†µã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. è¯¦ç»†çš„æµæ°´çº¿è°ƒè¯•
2. æ£€ç´¢è´¨é‡åˆ†æ
3. æ€§èƒ½ç›‘æ§
4. ç»“æœå¯¹æ¯”
5. å¯è§†åŒ–å±•ç¤º
"""

import time
import json
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
import pandas as pd

@dataclass
class RAGDebugResult:
    """RAGè°ƒè¯•ç»“æœæ•°æ®ç±»"""
    question: str
    timestamp: str
    retrieval_time: float
    llm_time: float
    total_time: float
    retrieved_docs: List[Any]
    context: str
    final_answer: str
    retrieval_scores: List[float]
    embedding_vector: List[float]
    prompt_tokens: int
    response_tokens: int
    
class RAGDebugger:
    """RAGç³»ç»Ÿè°ƒè¯•å™¨"""
    
    def __init__(self, vector_store, embeddings, llm, prompt_template):
        self.vector_store = vector_store
        self.embeddings = embeddings
        self.llm = llm
        self.prompt_template = prompt_template
        self.debug_history = []
        
    def comprehensive_debug(self, question: str, k: int = 4, verbose: bool = True) -> RAGDebugResult:
        """
        å…¨é¢çš„RAGè°ƒè¯•åˆ†æ
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            k: æ£€ç´¢æ–‡æ¡£æ•°é‡
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            
        Returns:
            RAGDebugResult: åŒ…å«æ‰€æœ‰è°ƒè¯•ä¿¡æ¯çš„ç»“æœå¯¹è±¡
        """
        start_time = time.time()
        timestamp = datetime.now().isoformat()
        
        if verbose:
            print("\n" + "="*80)
            print(f"ğŸ” RAG å…¨é¢è°ƒè¯•åˆ†æ - {timestamp}")
            print("="*80)
            print(f"ğŸ“ é—®é¢˜: {question}")
            print(f"ğŸ”¢ æ£€ç´¢æ•°é‡: {k}")
        
        # 1. é—®é¢˜å‘é‡åŒ–
        if verbose:
            print("\nğŸ”¢ æ­¥éª¤1: é—®é¢˜å‘é‡åŒ–")
        embedding_start = time.time()
        try:
            question_embedding = self.embeddings.embed_query(question)
            embedding_time = time.time() - embedding_start
            if verbose:
                print(f"  âœ… å‘é‡ç»´åº¦: {len(question_embedding)}")
                print(f"  â±ï¸ å‘é‡åŒ–è€—æ—¶: {embedding_time:.3f}ç§’")
                print(f"  ğŸ“Š å‘é‡èŒƒå›´: [{min(question_embedding):.4f}, {max(question_embedding):.4f}]")
        except Exception as e:
            if verbose:
                print(f"  âŒ å‘é‡åŒ–å¤±è´¥: {e}")
            question_embedding = []
            embedding_time = 0
        
        # 2. æ–‡æ¡£æ£€ç´¢
        if verbose:
            print("\nğŸ” æ­¥éª¤2: æ–‡æ¡£æ£€ç´¢")
        retrieval_start = time.time()
        try:
            # è·å–å¸¦åˆ†æ•°çš„æ£€ç´¢ç»“æœ
            docs_with_scores = self.vector_store.similarity_search_with_score(question, k=k)
            retrieved_docs = [doc for doc, score in docs_with_scores]
            retrieval_scores = [score for doc, score in docs_with_scores]
            retrieval_time = time.time() - retrieval_start
            
            if verbose:
                print(f"  âœ… æ£€ç´¢åˆ° {len(retrieved_docs)} ä¸ªæ–‡æ¡£")
                print(f"  â±ï¸ æ£€ç´¢è€—æ—¶: {retrieval_time:.3f}ç§’")
                print(f"  ğŸ“Š ç›¸ä¼¼åº¦åˆ†æ•°èŒƒå›´: [{min(retrieval_scores):.4f}, {max(retrieval_scores):.4f}]")
                
                for i, (doc, score) in enumerate(docs_with_scores):
                    print(f"\n    ğŸ“„ æ–‡æ¡£ {i+1} (åˆ†æ•°: {score:.4f}):")
                    print(f"      å†…å®¹é•¿åº¦: {len(doc.page_content)} å­—ç¬¦")
                    print(f"      å†…å®¹é¢„è§ˆ: {doc.page_content[:100]}...")
                    if hasattr(doc, 'metadata') and doc.metadata:
                        print(f"      å…ƒæ•°æ®: {doc.metadata}")
                        
        except Exception as e:
            if verbose:
                print(f"  âŒ æ£€ç´¢å¤±è´¥: {e}")
            retrieved_docs = []
            retrieval_scores = []
            retrieval_time = 0
        
        # 3. ä¸Šä¸‹æ–‡æ„å»º
        if verbose:
            print("\nğŸ“‹ æ­¥éª¤3: ä¸Šä¸‹æ–‡æ„å»º")
        try:
            context = "\n\n".join([doc.page_content for doc in retrieved_docs])
            if verbose:
                print(f"  âœ… ä¸Šä¸‹æ–‡æ€»é•¿åº¦: {len(context)} å­—ç¬¦")
                print(f"  ğŸ“ ä¸Šä¸‹æ–‡é¢„è§ˆ: {context[:200]}...")
        except Exception as e:
            if verbose:
                print(f"  âŒ ä¸Šä¸‹æ–‡æ„å»ºå¤±è´¥: {e}")
            context = ""
        
        # 4. æç¤ºæ„å»º
        if verbose:
            print("\nğŸ’¬ æ­¥éª¤4: æç¤ºæ„å»º")
        try:
            final_prompt = self.prompt_template.format_messages(
                context=context,
                input=question
            )
            prompt_content = "\n".join([msg.content for msg in final_prompt])
            prompt_tokens = len(prompt_content.split())  # ç®€å•çš„tokenä¼°ç®—
            
            if verbose:
                print(f"  âœ… æç¤ºæ¶ˆæ¯æ•°é‡: {len(final_prompt)}")
                print(f"  ğŸ“Š ä¼°ç®—tokenæ•°: {prompt_tokens}")
                print(f"  ğŸ“ æç¤ºé¢„è§ˆ: {prompt_content[:200]}...")
        except Exception as e:
            if verbose:
                print(f"  âŒ æç¤ºæ„å»ºå¤±è´¥: {e}")
            final_prompt = []
            prompt_tokens = 0
        
        # 5. LLMç”Ÿæˆ
        if verbose:
            print("\nğŸ¤– æ­¥éª¤5: LLMç”Ÿæˆ")
        llm_start = time.time()
        try:
            response = self.llm.invoke(final_prompt)
            llm_time = time.time() - llm_start
            
            if hasattr(response, 'content'):
                final_answer = response.content
            else:
                final_answer = str(response)
                
            response_tokens = len(final_answer.split())  # ç®€å•çš„tokenä¼°ç®—
            
            if verbose:
                print(f"  âœ… ç”ŸæˆæˆåŠŸ")
                print(f"  â±ï¸ ç”Ÿæˆè€—æ—¶: {llm_time:.3f}ç§’")
                print(f"  ğŸ“Š å“åº”tokenæ•°: {response_tokens}")
                print(f"  ğŸ“ ç­”æ¡ˆ: {final_answer}")
                
        except Exception as e:
            if verbose:
                print(f"  âŒ LLMç”Ÿæˆå¤±è´¥: {e}")
            final_answer = "ç”Ÿæˆå¤±è´¥"
            llm_time = 0
            response_tokens = 0
        
        total_time = time.time() - start_time
        
        # åˆ›å»ºè°ƒè¯•ç»“æœ
        debug_result = RAGDebugResult(
            question=question,
            timestamp=timestamp,
            retrieval_time=retrieval_time,
            llm_time=llm_time,
            total_time=total_time,
            retrieved_docs=retrieved_docs,
            context=context,
            final_answer=final_answer,
            retrieval_scores=retrieval_scores,
            embedding_vector=question_embedding,
            prompt_tokens=prompt_tokens,
            response_tokens=response_tokens
        )
        
        # ä¿å­˜åˆ°å†å²è®°å½•
        self.debug_history.append(debug_result)
        
        if verbose:
            print("\nğŸ“Š æ€§èƒ½æ€»ç»“:")
            print(f"  â±ï¸ æ€»è€—æ—¶: {total_time:.3f}ç§’")
            print(f"  ğŸ” æ£€ç´¢è€—æ—¶: {retrieval_time:.3f}ç§’ ({retrieval_time/total_time*100:.1f}%)")
            print(f"  ğŸ¤– LLMè€—æ—¶: {llm_time:.3f}ç§’ ({llm_time/total_time*100:.1f}%)")
            print(f"  ğŸ“Š Tokenä½¿ç”¨: {prompt_tokens + response_tokens}")
            print("="*80)
        
        return debug_result
    
    def analyze_retrieval_quality(self, question: str, k_values: List[int] = [1, 3, 5, 10]) -> Dict[str, Any]:
        """
        åˆ†æä¸åŒkå€¼ä¸‹çš„æ£€ç´¢è´¨é‡
        
        Args:
            question: æµ‹è¯•é—®é¢˜
            k_values: ä¸åŒçš„kå€¼åˆ—è¡¨
            
        Returns:
            åŒ…å«åˆ†æç»“æœçš„å­—å…¸
        """
        print(f"\nğŸ§ª æ£€ç´¢è´¨é‡åˆ†æ - é—®é¢˜: {question}")
        print("="*60)
        
        results = {}
        
        for k in k_values:
            print(f"\nğŸ“Š k={k} çš„æ£€ç´¢åˆ†æ:")
            print("-"*30)
            
            try:
                docs_with_scores = self.vector_store.similarity_search_with_score(question, k=k)
                scores = [score for doc, score in docs_with_scores]
                
                # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                avg_score = sum(scores) / len(scores) if scores else 0
                min_score = min(scores) if scores else 0
                max_score = max(scores) if scores else 0
                score_range = max_score - min_score if scores else 0
                
                results[k] = {
                    'num_docs': len(docs_with_scores),
                    'scores': scores,
                    'avg_score': avg_score,
                    'min_score': min_score,
                    'max_score': max_score,
                    'score_range': score_range,
                    'docs': [doc.page_content for doc, score in docs_with_scores]
                }
                
                print(f"  ğŸ“„ æ–‡æ¡£æ•°é‡: {len(docs_with_scores)}")
                print(f"  ğŸ“Š å¹³å‡åˆ†æ•°: {avg_score:.4f}")
                print(f"  ğŸ“ˆ åˆ†æ•°èŒƒå›´: {min_score:.4f} - {max_score:.4f}")
                print(f"  ğŸ“ åˆ†æ•°è·¨åº¦: {score_range:.4f}")
                
                # æ˜¾ç¤ºå‰3ä¸ªæ–‡æ¡£çš„ç®€è¦ä¿¡æ¯
                for i, (doc, score) in enumerate(docs_with_scores[:3]):
                    print(f"    {i+1}. åˆ†æ•°: {score:.4f}, å†…å®¹: {doc.page_content[:50]}...")
                    
            except Exception as e:
                print(f"  âŒ k={k} æ£€ç´¢å¤±è´¥: {e}")
                results[k] = {'error': str(e)}
        
        return results
    
    def compare_questions(self, questions: List[str], k: int = 4) -> pd.DataFrame:
        """
        æ¯”è¾ƒå¤šä¸ªé—®é¢˜çš„RAGæ€§èƒ½
        
        Args:
            questions: é—®é¢˜åˆ—è¡¨
            k: æ£€ç´¢æ–‡æ¡£æ•°é‡
            
        Returns:
            åŒ…å«æ¯”è¾ƒç»“æœçš„DataFrame
        """
        print(f"\nğŸ”„ å¤šé—®é¢˜æ€§èƒ½æ¯”è¾ƒ (k={k})")
        print("="*60)
        
        comparison_data = []
        
        for i, question in enumerate(questions):
            print(f"\nğŸ“ é—®é¢˜ {i+1}: {question[:50]}...")
            
            result = self.comprehensive_debug(question, k=k, verbose=False)
            
            comparison_data.append({
                'é—®é¢˜': question[:50] + '...' if len(question) > 50 else question,
                'æ£€ç´¢æ—¶é—´(ç§’)': result.retrieval_time,
                'LLMæ—¶é—´(ç§’)': result.llm_time,
                'æ€»æ—¶é—´(ç§’)': result.total_time,
                'æ£€ç´¢æ–‡æ¡£æ•°': len(result.retrieved_docs),
                'å¹³å‡ç›¸ä¼¼åº¦': sum(result.retrieval_scores) / len(result.retrieval_scores) if result.retrieval_scores else 0,
                'ä¸Šä¸‹æ–‡é•¿åº¦': len(result.context),
                'ç­”æ¡ˆé•¿åº¦': len(result.final_answer),
                'Prompt Tokens': result.prompt_tokens,
                'Response Tokens': result.response_tokens
            })
            
            print(f"  â±ï¸ æ€»è€—æ—¶: {result.total_time:.3f}ç§’")
            print(f"  ğŸ“Š å¹³å‡ç›¸ä¼¼åº¦: {sum(result.retrieval_scores) / len(result.retrieval_scores) if result.retrieval_scores else 0:.4f}")
        
        df = pd.DataFrame(comparison_data)
        print("\nğŸ“Š æ€§èƒ½æ¯”è¾ƒè¡¨:")
        print(df.to_string(index=False))
        
        return df
    
    def get_debug_summary(self) -> Dict[str, Any]:
        """
        è·å–è°ƒè¯•å†å²çš„ç»Ÿè®¡æ‘˜è¦
        
        Returns:
            åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        if not self.debug_history:
            return {"message": "æš‚æ— è°ƒè¯•å†å²"}
        
        total_queries = len(self.debug_history)
        avg_retrieval_time = sum(r.retrieval_time for r in self.debug_history) / total_queries
        avg_llm_time = sum(r.llm_time for r in self.debug_history) / total_queries
        avg_total_time = sum(r.total_time for r in self.debug_history) / total_queries
        avg_docs_retrieved = sum(len(r.retrieved_docs) for r in self.debug_history) / total_queries
        
        return {
            "æ€»æŸ¥è¯¢æ¬¡æ•°": total_queries,
            "å¹³å‡æ£€ç´¢æ—¶é—´": f"{avg_retrieval_time:.3f}ç§’",
            "å¹³å‡LLMæ—¶é—´": f"{avg_llm_time:.3f}ç§’",
            "å¹³å‡æ€»æ—¶é—´": f"{avg_total_time:.3f}ç§’",
            "å¹³å‡æ£€ç´¢æ–‡æ¡£æ•°": f"{avg_docs_retrieved:.1f}",
            "æœ€è¿‘æŸ¥è¯¢æ—¶é—´": self.debug_history[-1].timestamp
        }
    
    def export_debug_history(self, filename: str = None) -> str:
        """
        å¯¼å‡ºè°ƒè¯•å†å²åˆ°JSONæ–‡ä»¶
        
        Args:
            filename: æ–‡ä»¶åï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
            
        Returns:
            å¯¼å‡ºçš„æ–‡ä»¶è·¯å¾„
        """
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"rag_debug_history_{timestamp}.json"
        
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        export_data = []
        for result in self.debug_history:
            export_data.append({
                "question": result.question,
                "timestamp": result.timestamp,
                "retrieval_time": result.retrieval_time,
                "llm_time": result.llm_time,
                "total_time": result.total_time,
                "num_retrieved_docs": len(result.retrieved_docs),
                "context_length": len(result.context),
                "final_answer": result.final_answer,
                "retrieval_scores": result.retrieval_scores,
                "prompt_tokens": result.prompt_tokens,
                "response_tokens": result.response_tokens
            })
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“ è°ƒè¯•å†å²å·²å¯¼å‡ºåˆ°: {filename}")
        return filename

# ä¾¿æ·å‡½æ•°
def quick_debug_rag(question: str, vector_store, embeddings, llm, prompt_template, k: int = 4):
    """
    å¿«é€Ÿè°ƒè¯•RAGç³»ç»Ÿçš„ä¾¿æ·å‡½æ•°
    
    Args:
        question: ç”¨æˆ·é—®é¢˜
        vector_store: å‘é‡å­˜å‚¨
        embeddings: åµŒå…¥æ¨¡å‹
        llm: è¯­è¨€æ¨¡å‹
        prompt_template: æç¤ºæ¨¡æ¿
        k: æ£€ç´¢æ–‡æ¡£æ•°é‡
    """
    debugger = RAGDebugger(vector_store, embeddings, llm, prompt_template)
    return debugger.comprehensive_debug(question, k=k)

def analyze_rag_performance(questions: List[str], vector_store, embeddings, llm, prompt_template, k: int = 4):
    """
    åˆ†æRAGç³»ç»Ÿåœ¨å¤šä¸ªé—®é¢˜ä¸Šçš„æ€§èƒ½
    
    Args:
        questions: é—®é¢˜åˆ—è¡¨
        vector_store: å‘é‡å­˜å‚¨
        embeddings: åµŒå…¥æ¨¡å‹
        llm: è¯­è¨€æ¨¡å‹
        prompt_template: æç¤ºæ¨¡æ¿
        k: æ£€ç´¢æ–‡æ¡£æ•°é‡
        
    Returns:
        æ€§èƒ½åˆ†æDataFrame
    """
    debugger = RAGDebugger(vector_store, embeddings, llm, prompt_template)
    return debugger.compare_questions(questions, k=k)

if __name__ == "__main__":
    print("RAGè°ƒè¯•å·¥å…·é›†å·²åŠ è½½")
    print("ä¸»è¦åŠŸèƒ½:")
    print("1. RAGDebugger - å®Œæ•´çš„RAGè°ƒè¯•å™¨")
    print("2. quick_debug_rag - å¿«é€Ÿè°ƒè¯•å‡½æ•°")
    print("3. analyze_rag_performance - æ€§èƒ½åˆ†æå‡½æ•°")