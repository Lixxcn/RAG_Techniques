{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/context_enrichment_window_around_chunk_with_llamaindex.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文档检索的上下文丰富窗口\n",
    "\n",
    "## 概述\n",
    "\n",
    "此代码实现了向量数据库中用于文档检索的上下文丰富窗口技术。它通过向每个检索到的块添加周围的上下文来增强标准检索过程，从而提高返回信息的连贯性和完整性。\n",
    "\n",
    "## 动机\n",
    "\n",
    "传统的向量搜索通常返回孤立的文本块，这可能缺乏充分理解所需的必要上下文。这种方法旨在通过包含相邻的文本块来提供对检索信息的更全面的视图。\n",
    "\n",
    "## 关键组件\n",
    "\n",
    "1. PDF 处理和文本分块\n",
    "2. 使用 FAISS 和 OpenAI 嵌入创建向量存储\n",
    "3. 带有上下文窗口的自定义检索功能\n",
    "4. 标准检索与上下文丰富检索的比较\n",
    "\n",
    "## 方法详情\n",
    "\n",
    "### 文档预处理\n",
    "\n",
    "1. 读取 PDF 并将其转换为字符串。\n",
    "2. 文本被分割成带有周围句子的块\n",
    "\n",
    "### 向量存储创建\n",
    "\n",
    "1. 使用 OpenAI 嵌入来创建块的向量表示。\n",
    "2. 从这些嵌入创建一个 FAISS 向量存储。\n",
    "\n",
    "### 上下文丰富的检索\n",
    "\n",
    "LlamaIndex 有一个专门用于此类任务的解析器。[SentenceWindowNodeParser](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules/#sentencewindownodeparser) 这个解析器将文档分割成句子。但是生成的节点包含了具有关系结构的周围句子。然后，在查询时 [MetadataReplacementPostProcessor](https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/node_postprocessors/#metadatareplacementpostprocessor) 帮助连接回这些相关的句子。\n",
    "\n",
    "### 检索比较\n",
    "\n",
    "该笔记本包括一个比较标准检索与上下文丰富方法的章节。\n",
    "\n",
    "## 这种方法的好处\n",
    "\n",
    "1. 提供更连贯和上下文更丰富的结果\n",
    "2. 在减轻其返回孤立文本片段的倾向的同时，保持了向量搜索的优势\n",
    "3. 允许灵活调整上下文窗口的大小\n",
    "\n",
    "## 结论\n",
    "\n",
    "这种上下文丰富窗口技术为提高基于向量的文档搜索系统中检索信息的质量提供了一种有前途的方法。通过提供周围的上下文，它有助于保持检索信息的连贯性和完整性，可能有助于在问答等下游任务中实现更好的理解和更准确的响应。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "\n",
    "<img src=\"../images/vector-search-comparison_context_enrichment.svg\" alt=\"context enrichment window\" style=\"width:70%; height:auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 包安装和导入\n",
    "\n",
    "下面的单元格安装了运行此笔记本所需的所有必要软件包。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install faiss-cpu llama-index python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.readers import SimpleDirectoryReader\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser, SentenceSplitter\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.postprocessor import MetadataReplacementPostProcessor\n",
    "import faiss\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "\n",
    "# Original path append replaced for Colab compatibility\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Llamaindex global settings for llm and embeddings\n",
    "EMBED_DIMENSION=512\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\", dimensions=EMBED_DIMENSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required data files\n",
    "import os\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Download the PDF document used in this notebook\n",
    "!wget -O data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/\"\n",
    "reader = SimpleDirectoryReader(input_dir=path, required_exts=['.pdf'])\n",
    "documents = reader.load_data()\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建向量存储和检索器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FaisVectorStore to store embeddings\n",
    "fais_index = faiss.IndexFlatL2(EMBED_DIMENSION)\n",
    "vector_store = FaissVectorStore(faiss_index=fais_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 摄取管道"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用句子分割器的摄取管道"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pipeline = IngestionPipeline(\n",
    "    transformations=[SentenceSplitter()],\n",
    "    vector_store=vector_store\n",
    ")\n",
    "\n",
    "base_nodes = base_pipeline.run(documents=documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingestion Pipeline with Sentence Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_parser = SentenceWindowNodeParser(\n",
    "    # How many sentences on both sides to capture. \n",
    "    # Setting this to 3 results in 7 sentences.\n",
    "    window_size=3,\n",
    "    # the metadata key for to be used in MetadataReplacementPostProcessor\n",
    "    window_metadata_key=\"window\",\n",
    "    # the metadata key that holds the original sentence\n",
    "    original_text_metadata_key=\"original_sentence\"\n",
    ")\n",
    "\n",
    "# Create a pipeline with defined document transformations and vectorstore\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[node_parser],\n",
    "    vector_store=vector_store,\n",
    ")\n",
    "\n",
    "windowed_nodes = pipeline.run(documents=documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Explain the role of deforestation and fossil fuels in climate change\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying *without* Metadata Replacement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector index from base nodes\n",
    "base_index = VectorStoreIndex(base_nodes)\n",
    "\n",
    "# Instantiate query engine from vector index\n",
    "base_query_engine = base_index.as_query_engine(\n",
    "    similarity_top_k=1,\n",
    ")\n",
    "\n",
    "# Send query to the engine to get related node(s)\n",
    "base_response = base_query_engine.query(query)\n",
    "\n",
    "print(base_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print Metadata of the Retrieved Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(base_response.source_nodes[0].node.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying with Metadata Replacement\n",
    "\"Metadata replacement\" intutively might sound a little off topic since we're working on the base sentences. But LlamaIndex stores these \"before/after sentences\" in the metadata data of the nodes. Therefore to build back up these windows of sentences we need Metadata replacement post processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create window index from nodes created from SentenceWindowNodeParser\n",
    "windowed_index = VectorStoreIndex(windowed_nodes)\n",
    "\n",
    "# Instantiate query enine with MetadataReplacementPostProcessor\n",
    "windowed_query_engine = windowed_index.as_query_engine(\n",
    "    similarity_top_k=1,\n",
    "    node_postprocessors=[\n",
    "        MetadataReplacementPostProcessor(\n",
    "            target_metadata_key=\"window\" # `window_metadata_key` key defined in SentenceWindowNodeParser\n",
    "            )\n",
    "        ],\n",
    ")\n",
    "\n",
    "# Send query to the engine to get related node(s)\n",
    "windowed_response = windowed_query_engine.query(query)\n",
    "\n",
    "print(windowed_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print Metadata of the Retrieved Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window and original sentence are added to the metadata\n",
    "pprint(windowed_response.source_nodes[0].node.metadata)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
