{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/hierarchical_indices.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文档检索中的分层索引\n",
    "\n",
    "## 概述\n",
    "\n",
    "此代码实现了一个用于文档检索的分层索引系统，利用两个编码级别：文档级摘要和详细块。这种方法旨在通过首先通过摘要识别相关文档部分，然后深入到这些部分内的具体细节，来提高信息检索的效率和相关性。\n",
    "\n",
    "## 动机\n",
    "\n",
    "传统的平面索引方法在处理大型文档或语料库时可能会遇到困难，可能会遗漏上下文或返回不相关的信息。分层索引通过创建两层搜索系统来解决这个问题，允许更高效和上下文感知的检索。\n",
    "\n",
    "## 关键组件\n",
    "\n",
    "1. PDF处理和文本分块\n",
    "2. 使用OpenAI的GPT-4进行异步文档摘要\n",
    "3. 使用FAISS和OpenAI嵌入为摘要和详细块创建向量存储\n",
    "4. 自定义分层检索函数\n",
    "\n",
    "## 方法详情\n",
    "\n",
    "### 文档预处理和编码\n",
    "\n",
    "1. 加载PDF并将其分割成文档（可能按页面）。\n",
    "2. 使用GPT-4异步总结每个文档。\n",
    "3. 原始文档也被分割成更小的详细块。\n",
    "4. 创建两个独立的向量存储：\n",
    "   - 一个用于文档级摘要\n",
    "   - 一个用于详细块\n",
    "\n",
    "### 异步处理和速率限制\n",
    "\n",
    "1. 代码使用异步编程（asyncio）来提高效率。\n",
    "2. 实现批处理和指数退避来处理API速率限制。\n",
    "\n",
    "### 分层检索\n",
    "\n",
    "`retrieve_hierarchical`函数实现两层搜索：\n",
    "\n",
    "1. 它首先搜索摘要向量存储以识别相关文档部分。\n",
    "2. 对于每个相关摘要，然后搜索详细块向量存储，按相应页码过滤。\n",
    "3. 这种方法确保只从最相关的文档部分检索详细信息。\n",
    "\n",
    "## 此方法的优点\n",
    "\n",
    "1. 提高检索效率：通过首先搜索摘要，系统可以快速识别相关文档部分，而无需处理所有详细块。\n",
    "2. 更好的上下文保持：分层方法有助于维护检索信息的更广泛上下文。\n",
    "3. 可扩展性：这种方法对于大型文档或语料库特别有益，其中平面搜索可能效率低下或遗漏重要上下文。\n",
    "4. 灵活性：系统允许调整检索的摘要和块数量，为不同用例启用微调。\n",
    "\n",
    "## 实现细节\n",
    "\n",
    "1. 异步编程：利用Python的asyncio进行高效的I/O操作和API调用。\n",
    "2. 速率限制处理：实现批处理和指数退避以有效管理API速率限制。\n",
    "3. 持久存储：在本地保存生成的向量存储以避免不必要的重新计算。\n",
    "\n",
    "## 结论\n",
    "\n",
    "分层索引代表了一种复杂的文档检索方法，特别适用于大型或复杂的文档集。通过利用高级摘要和详细块，它在广泛的上下文理解和特定信息检索之间提供了平衡。这种方法在需要高效和上下文感知信息检索的各个领域都有潜在应用，如法律文档分析、学术研究或大规模内容管理系统。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "\n",
    "<img src=\"../images/hierarchical_indices.svg\" alt=\"hierarchical_indices\" style=\"width:50%; height:auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "\n",
    "<img src=\"../images/hierarchical_indices_example.svg\" alt=\"hierarchical_indices\" style=\"width:100%; height:auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 包安装和导入\n",
    "\n",
    "下面的单元格安装运行此notebook所需的所有必要包。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装所需包\n",
    "!pip install langchain langchain-openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 克隆仓库以访问辅助函数和评估模块\n",
    "!git clone https://github.com/NirDiamant/RAG_TECHNIQUES.git\n",
    "import sys\n",
    "sys.path.append('RAG_TECHNIQUES')\n",
    "# 如果需要使用最新数据运行\n",
    "# !cp -r RAG_TECHNIQUES/data ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\N7\\PycharmProjects\\llm_tasks\\RAG_TECHNIQUES\\.venv\\Lib\\site-packages\\deepeval\\__init__.py:45: UserWarning: You are using deepeval version 0.21.73, however version 1.0.3 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.summarize.chain import load_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# 为Colab兼容性替换原始路径追加\n",
    "from helper_functions import *\n",
    "from evaluation.evalute_rag import *\n",
    "from helper_functions import encode_pdf, encode_from_string\n",
    "\n",
    "# 从.env文件加载环境变量\n",
    "load_dotenv()\n",
    "\n",
    "# 设置OpenAI API密钥环境变量\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义文档路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载所需数据文件\n",
    "import os\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# 下载此notebook中使用的PDF文档\n",
    "!wget -O data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n",
    "!wget -O data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/Understanding_Climate_Change.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编码到摘要和块级别的函数，共享页面元数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def encode_pdf_hierarchical(path, chunk_size=1000, chunk_overlap=200, is_string=False):\n",
    "    \"\"\"\n",
    "    Asynchronously encodes a PDF book into a hierarchical vector store using OpenAI embeddings.\n",
    "    Includes rate limit handling with exponential backoff.\n",
    "    \n",
    "    Args:\n",
    "        path: The path to the PDF file.\n",
    "        chunk_size: The desired size of each text chunk.\n",
    "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple containing two FAISS vector stores:\n",
    "        1. Document-level summaries\n",
    "        2. Detailed chunks\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load PDF documents\n",
    "    if not is_string:\n",
    "        loader = PyPDFLoader(path)\n",
    "        documents = await asyncio.to_thread(loader.load)\n",
    "    else:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            # Set a really small chunk size, just to show.\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False,\n",
    "        )\n",
    "        documents = text_splitter.create_documents([path])\n",
    "\n",
    "\n",
    "    #"# 创建文档级摘要\n",
    "    summary_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\", max_tokens=4000)\n",
    "    summary_chain = load_summarize_chain(summary_llm, chain_type=\"map_reduce\")\n",
    "    \n",
    "    async def summarize_doc(doc):\n",
    "        \"\"\"\n",
    "        "使用速率限制处理总结单个文档。\n",
    "        \n",
    "        Args:\n",
    "            doc: The document to be summarized.\n",
    "            \n",
    "        Returns:\n",
    "            A summarized Document object.\n",
    "        \"\"\"\n",
    "        #"# 使用指数退避重试总结\n",
    "        summary_output = await retry_with_exponential_backoff(summary_chain.ainvoke([doc]))\n",
    "        summary = summary_output['output_text']\n",
    "        return Document(\n",
    "            page_content=summary,\n",
    "            metadata={\"source\": path, \"page\": doc.metadata[\"page\"], \"summary\": True}\n",
    "        )\n",
    "\n",
    "# 以较小批次处理文档以避免速率限制\n",
    "batch_size = 5  # 根据您的速率限制调整此值\n",
    "    summaries = []\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i+batch_size]\n",
    "        batch_summaries = await asyncio.gather(*[summarize_doc(doc) for doc in batch])\n",
    "        summaries.extend(batch_summaries)\n",
    "        "await asyncio.sleep(1)  # 批次之间的短暂暂停\n",
    "\n",
    "    #"# 将文档分割成详细块\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
    "    )\n",
    "    detailed_chunks = await asyncio.to_thread(text_splitter.split_documents, documents)\n",
    "\n",
    "    #"# 更新详细块的元数据\n",
    "    for i, chunk in enumerate(detailed_chunks):\n",
    "        chunk.metadata.update({\n",
    "            \"chunk_id\": i,\n",
    "            \"summary\": False,\n",
    "            \"page\": int(chunk.metadata.get(\"page\", 0))\n",
    "        })\n",
    "\n",
    "    #"# 创建嵌入\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# 使用速率限制处理异步创建向量存储\n",
    "    async def create_vectorstore(docs):\n",
    "        \"\"\"\n",
    "使用速率限制处理从文档列表创建向量存储。\n",
    "        \n",
    "        Args:\n",
    "            docs: The list of documents to be embedded.\n",
    "            \n",
    "        Returns:\n",
    "            A FAISS vector store containing the embedded documents.\n",
    "        \"\"\"\n",
    "        return await retry_with_exponential_backoff(\n",
    "            asyncio.to_thread(FAISS.from_documents, docs, embeddings)\n",
    "        )\n",
    "\n",
    "# 并发生成摘要和详细块的向量存储\n",
    "    summary_vectorstore, detailed_vectorstore = await asyncio.gather(\n",
    "        create_vectorstore(summaries),\n",
    "        create_vectorstore(detailed_chunks)\n",
    "    )\n",
    "\n",
    "    return summary_vectorstore, detailed_vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如果向量存储不存在，将PDF书籍编码为文档级摘要和详细块\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"../vector_stores/summary_store\") and os.path.exists(\"../vector_stores/detailed_store\"):\n",
    "   embeddings = OpenAIEmbeddings()\n",
    "   summary_store = FAISS.load_local(\"../vector_stores/summary_store\", embeddings, allow_dangerous_deserialization=True)\n",
    "   detailed_store = FAISS.load_local(\"../vector_stores/detailed_store\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "else:\n",
    "    summary_store, detailed_store = await encode_pdf_hierarchical(path)\n",
    "    summary_store.save_local(\"../vector_stores/summary_store\")\n",
    "    detailed_store.save_local(\"../vector_stores/detailed_store\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 根据摘要级别检索信息，然后从块级别向量存储检索信息并根据摘要级别页面过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_hierarchical(query, summary_vectorstore, detailed_vectorstore, k_summaries=3, k_chunks=5):\n",
    "    \"\"\"\n",
    "使用查询执行分层检索。\n",
    "\n",
    "    Args:\n",
    "        query: The search query.\n",
    "        summary_vectorstore: The vector store containing document summaries.\n",
    "        detailed_vectorstore: The vector store containing detailed chunks.\n",
    "        k_summaries: The number of top summaries to retrieve.\n",
    "        k_chunks: The number of detailed chunks to retrieve per summary.\n",
    "\n",
    "    Returns:\n",
    "        A list of relevant detailed chunks.\n",
    "    \"\"\"\n",
    "    \n",
    "    #"# 检索顶级摘要\n",
    "    top_summaries = summary_vectorstore.similarity_search(query, k=k_summaries)\n",
    "    \n",
    "    relevant_chunks = []\n",
    "    for summary in top_summaries:\n",
    "        #"# 对于每个摘要，检索相关的详细块\n",
    "        page_number = summary.metadata[\"page\"]\n",
    "        page_filter = lambda metadata: metadata[\"page\"] == page_number\n",
    "        page_chunks = detailed_vectorstore.similarity_search(\n",
    "            query, \n",
    "            k=k_chunks, \n",
    "            filter=page_filter\n",
    "        )\n",
    "        relevant_chunks.extend(page_chunks)\n",
    "    \n",
    "    return relevant_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在用例上演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the greenhouse effect?\"\n",
    "results = retrieve_hierarchical(query, summary_store, detailed_store)\n",
    "\n",
    "# 打印结果\n",
    "for chunk in results:\n",
    "    print(f\"Page: {chunk.metadata['page']}\")\n",
    "    print(f\"Content: {chunk.page_content}...\")  # 打印前100个字符\n",
    "    print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
