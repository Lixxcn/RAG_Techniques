{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/fusion_retrieval_with_llamaindex.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文档搜索中的融合检索\n",
    "\n",
    "## 概述\n",
    "\n",
    "此代码实现了一个融合检索系统，该系统结合了基于向量的相似性搜索和基于关键字的 BM25 检索。该方法旨在利用两种方法的优势，以提高文档检索的整体质量和相关性。\n",
    "\n",
    "## 动机\n",
    "\n",
    "传统的检索方法通常依赖于语义理解（基于向量）或关键字匹配（BM25）。每种方法都有其优点和缺点。融合检索旨在结合这些方法，以创建一个更强大、更准确的检索系统，能够有效处理更广泛的查询。\n",
    "\n",
    "## 关键组件\n",
    "\n",
    "1. PDF 处理和文本分块\n",
    "2. 使用 FAISS 和 OpenAI 嵌入创建向量存储\n",
    "3. 创建用于基于关键字检索的 BM25 索引\n",
    "4. 融合 BM25 和向量搜索结果以实现更好的检索\n",
    "\n",
    "## 方法详情\n",
    "\n",
    "### 文档预处理\n",
    "\n",
    "1. 使用 SentenceSplitter 加载 PDF 并将其拆分为块。\n",
    "2. 通过将 't' 替换为空格和清除换行符来清理块（可能解决了特定的格式问题）。\n",
    "\n",
    "### 向量存储创建\n",
    "\n",
    "1. 使用 OpenAI 嵌入创建文本块的向量表示。\n",
    "2. 从这些嵌入创建一个 FAISS 向量存储，以进行高效的相似性搜索。\n",
    "\n",
    "### BM25 索引创建\n",
    "\n",
    "1. 从用于向量存储的相同文本块创建 BM25 索引。\n",
    "2. 这允许与基于向量的方法一起进行基于关键字的检索。\n",
    "\n",
    "### 查询融合检索\n",
    "\n",
    "创建两个索引后，查询融合检索将它们结合起来以实现混合检索\n",
    "\n",
    "## 此方法的优点\n",
    "\n",
    "1. 提高检索质量：通过结合语义和基于关键字的搜索，系统可以捕获概念相似性和精确的关键字匹配。\n",
    "2. 灵活性：`retriever_weights` 参数允许根据特定的用例或查询类型调整向量和关键字搜索之间的平衡。\n",
    "3. 鲁棒性：组合方法可以有效处理更广泛的查询，减轻了单个方法的弱点。\n",
    "4. 可定制性：该系统可以轻松适应使用不同的向量存储或基于关键字的检索方法。\n",
    "\n",
    "## 结论\n",
    "\n",
    "融合检索代表了一种强大的文档搜索方法，它结合了语义理解和关键字匹配的优点。通过利用基于向量和 BM25 的检索方法，它为信息检索任务提供了更全面、更灵活的解决方案。这种方法在概念相似性和关键字相关性都很重要的各个领域都有潜在的应用，例如学术研究、法律文档搜索或通用搜索引擎。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package Installation and Imports\n",
    "\n",
    "The cell below installs all necessary packages required to run this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install faiss-cpu llama-index python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from typing import List\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.readers import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.schema import BaseNode, TransformComponent\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.legacy.retrievers.bm25_retriever import BM25Retriever\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "import faiss\n",
    "\n",
    "# Original path append replaced for Colab compatibility\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Llamaindex global settings for llm and embeddings\n",
    "EMBED_DIMENSION=512\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\", dimensions=EMBED_DIMENSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required data files\n",
    "import os\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Download the PDF document used in this notebook\n",
    "!wget -O data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/\"\n",
    "reader = SimpleDirectoryReader(input_dir=path, required_exts=['.pdf'])\n",
    "documents = reader.load_data()\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FaisVectorStore to store embeddings\n",
    "fais_index = faiss.IndexFlatL2(EMBED_DIMENSION)\n",
    "vector_store = FaissVectorStore(faiss_index=fais_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaner Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCleaner(TransformComponent):\n",
    "    \"\"\"\n",
    "    Transformation to be used within the ingestion pipeline.\n",
    "    Cleans clutters from texts.\n",
    "    \"\"\"\n",
    "    def __call__(self, nodes, **kwargs) -> List[BaseNode]:\n",
    "        \n",
    "        for node in nodes:\n",
    "            node.text = node.text.replace('\\t', ' ') # Replace tabs with spaces\n",
    "            node.text = node.text.replace(' \\n', ' ') # Replace paragprah seperator with spacaes\n",
    "            \n",
    "        return nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline instantiation with: \n",
    "# node parser, custom transformer, vector store and documents\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(),\n",
    "        TextCleaner()\n",
    "    ],\n",
    "    vector_store=vector_store,\n",
    "    documents=documents\n",
    ")\n",
    "\n",
    "# Run the pipeline to get nodes\n",
    "nodes = pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25 Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    nodes=nodes,\n",
    "    similarity_top_k=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex(nodes)\n",
    "vector_retriever = index.as_retriever(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fusing Both Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = QueryFusionRetriever(\n",
    "    retrievers=[\n",
    "        vector_retriever,\n",
    "        bm25_retriever\n",
    "    ],\n",
    "    retriever_weights=[\n",
    "        0.6, # vector retriever weight\n",
    "        0.4 # BM25 retriever weight\n",
    "    ],\n",
    "    num_queries=1, \n",
    "    mode='dist_based_score',\n",
    "    use_async=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About parameters\n",
    "\n",
    "1. `num_queries`:  Query Fusion Retriever not only combines retrievers but also can genereate multiple questions from a given query. This parameter controls how many total queries will be passed to the retrievers. Therefore setting it to 1 disables query generation and the final retriever only uses the initial query.\n",
    "2. `mode`: There are 4 options for this parameter. \n",
    "   - **reciprocal_rerank**: Applies reciporical ranking. (Since there is no normalization, this method is not suitable for this kind of application. Beacuse different retrirevers will return score scales)\n",
    "   - **relative_score**: Applies MinMax based on the min and max scores among all the nodes. Then scaled to be between 0 and 1. Finally scores are weighted by the relative retrievers based on `retriever_weights`.  \n",
    "      ```math\n",
    "      min\\_score = min(scores)\n",
    "      \\\\ max\\_score = max(scores)\n",
    "      ```\n",
    "   - **dist_based_score**:  Only difference from `relative_score` is the MinMax sclaing is based on mean and std of the scores. Scaling and weighting is the same.\n",
    "      ```math\n",
    "       min\\_score = mean\\_score - 3 * std\\_dev\n",
    "      \\\\ max\\_score = mean\\_score + 3 * std\\_dev\n",
    "      ```\n",
    "   - **simple**: This method is simply takes the max score of each chunk.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Case example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query\n",
    "query = \"What are the impacts of climate change on the environment?\"\n",
    "\n",
    "# Perform fusion retrieval\n",
    "response = retriever.retrieve(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Final Retrieved Nodes with Scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in response:\n",
    "    print(f\"Node Score: {node.score:.2}\")\n",
    "    print(f\"Node Content: {node.text}\")\n",
    "    print(\"-\"*100)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
