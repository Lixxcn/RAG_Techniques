{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/HyPE_Hypothetical_Prompt_Embeddings.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 假设性提示嵌入 (HyPE)\n",
    "\n",
    "## 概述\n",
    "\n",
    "此代码实现了一个由假设性提示嵌入 (HyPE) 增强的检索增强生成 (RAG) 系统。与传统的RAG管道在查询-文档风格不匹配方面存在困难不同，HyPE在索引阶段预计算假设性问题。这将检索转换为问题-问题匹配问题，消除了对昂贵的运行时查询扩展技术的需求。\n",
    "\n",
    "## 笔记本的关键组件\n",
    "\n",
    "1. PDF处理和文本提取\n",
    "2. 文本分块以保持连贯的信息单元\n",
    "3. **假设性提示嵌入生成** 使用LLM为每个块创建多个代理问题\n",
    "4. 使用 [FAISS](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/) 和 OpenAI 嵌入创建向量存储\n",
    "5. 用于查询处理文档的检索器设置\n",
    "6. RAG系统的评估\n",
    "\n",
    "## 方法详情\n",
    "\n",
    "### 文档预处理\n",
    "\n",
    "1. 使用 `PyPDFLoader` 加载PDF。\n",
    "2. 使用 `RecursiveCharacterTextSplitter` 将文本分割成块，具有指定的块大小和重叠。\n",
    "\n",
    "### 假设性问题生成\n",
    "\n",
    "HyPE不是嵌入原始文本块，而是**为每个块生成多个假设性提示**。这些**预计算的问题**模拟用户查询，改善与真实世界搜索的对齐。这消除了像HyDE等技术中需要的运行时合成答案生成的需求。\n",
    "\n",
    "### 向量存储创建\n",
    "\n",
    "1. 每个假设性问题都使用OpenAI嵌入进行嵌入。\n",
    "2. 构建FAISS向量存储，**将每个问题嵌入与其原始块关联**。\n",
    "3. 这种方法**为每个块存储多个表示**，增加检索灵活性。\n",
    "\n",
    "### 检索器设置\n",
    "\n",
    "1. 检索器针对**问题-问题匹配**而不是直接文档检索进行优化。\n",
    "2. FAISS索引在假设性提示嵌入上实现**高效的最近邻**搜索。\n",
    "3. 检索到的块为下游LLM生成提供**更丰富和更精确的上下文**。\n",
    "\n",
    "## 关键特性\n",
    "\n",
    "1. **预计算假设性提示** – 在没有运行时开销的情况下改善查询对齐。\n",
    "2. **多向量表示** – 每个块被多次索引以获得更广泛的语义覆盖。\n",
    "3. **高效检索** – FAISS确保在增强嵌入上进行快速相似性搜索。\n",
    "4. **模块化设计** – 管道易于适应不同的数据集和检索设置。此外，它与大多数优化（如重排序等）兼容。\n",
    "\n",
    "## 评估\n",
    "\n",
    "HyPE的有效性在多个数据集上进行评估，显示：\n",
    "\n",
    "- 检索精度提高多达42个百分点\n",
    "- 声明召回率提高多达45个百分点\n",
    "    (在[预印本](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5139335)中查看完整评估结果)\n",
    "\n",
    "## 此方法的优点\n",
    "\n",
    "1. **消除查询时开销** – 所有假设性生成都在索引时离线完成。\n",
    "2. **增强检索精度** – 查询和存储内容之间更好的对齐。\n",
    "3. **可扩展且高效** – 没有额外的每查询计算成本；检索与标准RAG一样快。\n",
    "4. **灵活且可扩展** – 可以与高级RAG技术（如重排序）结合使用。\n",
    "\n",
    "## 结论\n",
    "\n",
    "HyPE为传统RAG系统提供了一个可扩展且高效的替代方案，克服了查询-文档风格不匹配，同时避免了运行时查询扩展的计算成本。通过将假设性提示生成移至索引阶段，它显著增强了检索精度和效率，使其成为现实世界应用的实用解决方案。\n",
    "\n",
    "有关更多详细信息，请参阅完整论文：[预印本](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5139335)\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "\n",
    "<img src=\"../images/hype.svg\" alt=\"HyPE\" style=\"width:70%; height:auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 包安装和导入\n",
    "\n",
    "下面的单元格安装了运行此笔记本所需的所有必要软件包。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装所需的包\n",
    "!pip install faiss-cpu futures langchain-community python-dotenv tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 克隆存储库以访问辅助函数和评估模块\n",
    "!git clone https://github.com/NirDiamant/RAG_TECHNIQUES.git\n",
    "import sys\n",
    "sys.path.append('RAG_TECHNIQUES')\n",
    "# 如果您需要使用最新数据运行\n",
    "# !cp -r RAG_TECHNIQUES/data ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "\n",
    "# 从 .env 文件加载环境变量\n",
    "load_dotenv()\n",
    "\n",
    "# 设置 OpenAI API 密钥环境变量（如果不使用 OpenAI 请注释掉）\n",
    "if not os.getenv('OPENAI_API_KEY'):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = input(\"请输入您的 OpenAI API 密钥: \")\n",
    "else:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# 为 Colab 兼容性替换了原始路径附加\n",
    "from helper_functions import *\n",
    "from evaluation.evalute_rag import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义常量\n",
    "\n",
    "- `PATH`: 要嵌入到 RAG 管道的数据的路径\n",
    "\n",
    "本教程使用 OpenAI 端点 ([可用模型](https://platform.openai.com/docs/pricing))。\n",
    "- `LANGUAGE_MODEL_NAME`: 要使用的语言模型的名称。\n",
    "- `EMBEDDING_MODEL_NAME`: 要使用的嵌入模型的名称。\n",
    "\n",
    "本教程使用 `RecursiveCharacterTextSplitter` 分块方法，其中使用的分块长度函数是 python `len` 函数。此处要调整的分块变量是：\n",
    "- `CHUNK_SIZE`: 一个块的最小长度\n",
    "- `CHUNK_OVERLAP`: 两个连续块的重叠部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required data files\n",
    "import os\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Download the PDF document used in this notebook\n",
    "!wget -O data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n",
    "!wget -O data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"data/Understanding_Climate_Change.pdf\"\n",
    "LANGUAGE_MODEL_NAME = \"gpt-4o-mini\"\n",
    "EMBEDDING_MODEL_NAME = \"text-embedding-3-small\"\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义假设性提示嵌入的生成\n",
    "\n",
    "下面的代码块为每个文本块生成假设性问题，并将其嵌入以供检索。\n",
    "\n",
    "- LLM 从输入块中提取关键问题。\n",
    "- 这些问题使用 OpenAI 的模型进行嵌入。\n",
    "- 该函数返回原始块及其提示嵌入，稍后用于检索。\n",
    "\n",
    "为确保输出干净，会删除多余的换行符，并且在需要时可以使用正则表达式解析来改进列表格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hypothetical_prompt_embeddings(chunk_text: str):\n",
    "    \"\"\"\n",
    "    Uses the LLM to generate multiple hypothetical questions for a single chunk.\n",
    "    These questions will be used as 'proxies' for the chunk during retrieval.\n",
    "\n",
    "    Parameters:\n",
    "    chunk_text (str): Text contents of the chunk\n",
    "\n",
    "    Returns:\n",
    "    chunk_text (str): Text contents of the chunk. This is done to make the \n",
    "        multithreading easier\n",
    "    hypothetical prompt embeddings (List[float]): A list of embedding vectors\n",
    "        generated from the questions\n",
    "    \"\"\"\n",
    "    llm = ChatOpenAI(temperature=0, model_name=LANGUAGE_MODEL_NAME)\n",
    "    embedding_model = OpenAIEmbeddings(model=EMBEDDING_MODEL_NAME)\n",
    "\n",
    "    question_gen_prompt = PromptTemplate.from_template(\n",
    "        \"Analyze the input text and generate essential questions that, when answered, \\\n",
    "        capture the main points of the text. Each question should be one line, \\\n",
    "        without numbering or prefixes.\\n\\n \\\n",
    "        Text:\\n{chunk_text}\\n\\nQuestions:\\n\"\n",
    "    )\n",
    "    question_chain = question_gen_prompt | llm | StrOutputParser()\n",
    "\n",
    "    # parse questions from response\n",
    "    # Notes: \n",
    "    # - gpt4o likes to split questions by \\n\\n so we remove one \\n\n",
    "    # - for production or if using smaller models from ollama, it's beneficial to use regex to parse \n",
    "    # things like (un)ordeed lists\n",
    "    # r\"^\\s*[\\-\\*\\•]|\\s*\\d+\\.\\s*|\\s*[a-zA-Z]\\)\\s*|\\s*\\(\\d+\\)\\s*|\\s*\\([a-zA-Z]\\)\\s*|\\s*\\([ivxlcdm]+\\)\\s*\"\n",
    "    questions = question_chain.invoke({\"chunk_text\": chunk_text}).replace(\"\\n\\n\", \"\\n\").split(\"\\n\")\n",
    "    \n",
    "    return chunk_text, embedding_model.embed_documents(questions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义 FAISS 向量存储的创建和填充\n",
    "\n",
    "下面的代码块通过并行嵌入文本块来构建 FAISS 向量存储。\n",
    "\n",
    "会发生什么？\n",
    "- 并行处理 – 使用线程更快地生成嵌入。\n",
    "- FAISS 初始化 – 设置 L2 索引以进行高效的相似性搜索。\n",
    "- 块嵌入 – 每个块存储多次，每个生成的问句嵌入一次。\n",
    "- 内存存储 – 使用 InMemoryDocstore 进行快速查找。\n",
    "\n",
    "这确保了高效的检索，通过预先计算的问句嵌入改进了查询对齐。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_vector_store(chunks: List[str]):\n",
    "    \"\"\"\n",
    "    Creates and populates a FAISS vector store from a list of text chunks.\n",
    "\n",
    "    This function processes a list of text chunks in parallel, generating \n",
    "    hypothetical prompt embeddings for each chunk.\n",
    "    The embeddings are stored in a FAISS index for efficient similarity search.\n",
    "\n",
    "    Parameters:\n",
    "    chunks (List[str]): A list of text chunks to be embedded and stored.\n",
    "\n",
    "    Returns:\n",
    "    FAISS: A FAISS vector store containing the embedded text chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    # Wait with initialization to see vector lengths\n",
    "    vector_store = None  \n",
    "\n",
    "    with ThreadPoolExecutor() as pool:  \n",
    "        # Use threading to speed up generation of prompt embeddings\n",
    "        futures = [pool.submit(generate_hypothetical_prompt_embeddings, c) for c in chunks]\n",
    "        \n",
    "        # Process embeddings as they complete\n",
    "        for f in tqdm(as_completed(futures), total=len(chunks)):  \n",
    "            \n",
    "            chunk, vectors = f.result()  # Retrieve the processed chunk and its embeddings\n",
    "            \n",
    "            # Initialize the FAISS vector store on the first chunk\n",
    "            if vector_store == None:  \n",
    "                vector_store = FAISS(\n",
    "                    embedding_function=OpenAIEmbeddings(model=EMBEDDING_MODEL_NAME),  # Define embedding model\n",
    "                    index=faiss.IndexFlatL2(len(vectors[0]))  # Define an L2 index for similarity search\n",
    "                    docstore=InMemoryDocstore(),  # Use in-memory document storage\n",
    "                    index_to_docstore_id={}  # Maintain index-to-document mapping\n",
    "                )\n",
    "            \n",
    "            # Pair the chunk's content with each generated embedding vector.\n",
    "            # Each chunk is inserted multiple times, once for each prompt vector\n",
    "            chunks_with_embedding_vectors = [(chunk.page_content, vec) for vec in vectors]\n",
    "            \n",
    "            # Add embeddings to the store\n",
    "            vector_store.add_embeddings(chunks_with_embedding_vectors)  \n",
    "\n",
    "    return vector_store  # Return the populated vector store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将 PDF 编码到 FAISS 向量存储中\n",
    "\n",
    "下面的代码块处理一个 PDF 文件并将其内容存储为嵌入以供检索。\n",
    "\n",
    "会发生什么？\n",
    "- PDF 加载 – 从文档中提取文本。\n",
    "- 分块 – 将文本分割成重叠的段落以更好地保留上下文。\n",
    "- 预处理 – 清理文本以提高嵌入质量。\n",
    "- 向量存储创建 – 生成嵌入并将其存储在 FAISS 中以供检索。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_pdf(path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Encodes a PDF book into a vector store using OpenAI embeddings.\n",
    "\n",
    "    Args:\n",
    "        path: The path to the PDF file.\n",
    "        chunk_size: The desired size of each text chunk.\n",
    "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "        A FAISS vector store containing the encoded book content.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load PDF documents\n",
    "    loader = PyPDFLoader(path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
    "    )\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    cleaned_texts = replace_t_with_space(texts)\n",
    "\n",
    "    vectorstore = prepare_vector_store(cleaned_texts)\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建 HyPE 向量存储\n",
    "\n",
    "现在我们处理 PDF 并存储其嵌入。\n",
    "此步骤使用编码后的文档初始化 FAISS 向量存储。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [00:22<00:00,  4.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# Chunk size can be quite large with HyPE as we are not loosing percision with more\n",
    "# information. For production, test how exhaustive your model is in generating sufficient \n",
    "# amount of questions per chunk. This will mostly depend on your information density.\n",
    "chunks_vector_store = encode_pdf(PATH, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建检索器\n",
    "\n",
    "现在我们设置检索器以从向量存储中获取相关块。\n",
    "\n",
    "基于查询相似度检索最相关的前 `k=3` 个块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_query_retriever = chunks_vector_store.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试检索器\n",
    "\n",
    "现在我们使用示例查询来测试检索。\n",
    "\n",
    "- 查询向量存储以找到最相关的块。\n",
    "- 对结果进行去重以删除可能重复的块。\n",
    "- 显示检索到的上下文以供检查。\n",
    "\n",
    "此步骤验证检索器是否为给定问题返回有意义和多样化的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context 1:\n",
      "Most of these climate changes are attributed to very small variations in Earth's orbit that \n",
      "change the amount of solar energy our planet receives. During the Holocene epoch, which \n",
      "began at the end of the last ice age, human societies f lourished, but the industrial era has seen \n",
      "unprecedented changes.  \n",
      "Modern Observations  \n",
      "Modern scientific observations indicate a rapid increase in global temperatures, sea levels, \n",
      "and extreme weather events. The Intergovernmental Panel on Climate Change (IPCC) has \n",
      "documented these changes extensively. Ice core samples, tree rings, and ocean sediments \n",
      "provide a historical record that scientists use to understand past climate conditions and \n",
      "predict future trends. The evidence overwhelmingly shows that recent changes are primarily \n",
      "driven by human activities, particularly the emission of greenhou se gases.  \n",
      "Chapter 2: Causes of Climate Change  \n",
      "Greenhouse Gases\n",
      "\n",
      "\n",
      "Context 2:\n",
      "driven by human activities, particularly the emission of greenhou se gases.  \n",
      "Chapter 2: Causes of Climate Change  \n",
      "Greenhouse Gases  \n",
      "The primary cause of recent climate change is the increase in greenhouse gases in the \n",
      "atmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous \n",
      "oxide (N2O), trap heat from the sun, creating a \"greenhouse effect.\" This effect is  essential \n",
      "for life on Earth, as it keeps the planet warm enough to support life. However, human \n",
      "activities have intensified this natural process, leading to a warmer climate.  \n",
      "Fossil Fuels  \n",
      "Burning fossil fuels for energy releases large amounts of CO2. This includes coal, oil, and \n",
      "natural gas used for electricity, heating, and transportation. The industrial revolution marked \n",
      "the beginning of a significant increase in fossil fuel consumption, which continues to rise \n",
      "today.  \n",
      "Coal\n",
      "\n",
      "\n",
      "Context 3:\n",
      "Understanding Climate Change  \n",
      "Chapter 1: Introduction to Climate Change  \n",
      "Climate change refers to significant, long -term changes in the global climate. The term \n",
      "\"global climate\" encompasses the planet's overall weather patterns, including temperature, \n",
      "precipitation, and wind patterns, over an extended period. Over the past cent ury, human \n",
      "activities, particularly the burning of fossil fuels and deforestation, have significantly \n",
      "contributed to climate change.  \n",
      "Historical Context  \n",
      "The Earth's climate has changed throughout history. Over the past 650,000 years, there have \n",
      "been seven cycles of glacial advance and retreat, with the abrupt end of the last ice age about \n",
      "11,700 years ago marking the beginning of the modern climate era and  human civilization. \n",
      "Most of these climate changes are attributed to very small variations in Earth's orbit that \n",
      "change the amount of solar energy our planet receives. During the Holocene epoch, which\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_query = \"What is the main cause of climate change?\"\n",
    "context = retrieve_context_per_question(test_query, chunks_query_retriever)\n",
    "context = list(set(context))\n",
    "show_context(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'questions': ['1. **Multiple Choice: Causes of Climate Change**',\n",
       "  '   - What is the primary cause of the current climate change trend?',\n",
       "  '     A) Solar radiation variations',\n",
       "  '     B) Natural cycles of the Earth',\n",
       "  '     C) Human activities, such as burning fossil fuels',\n",
       "  '     D) Volcanic eruptions',\n",
       "  '',\n",
       "  '2. **True or False: Impact on Biodiversity**',\n",
       "  '   - True or False: Climate change does not have any significant impact on the migration patterns and extinction rates of various species.',\n",
       "  '',\n",
       "  '3. **Short Answer: Mitigation Strategies**',\n",
       "  '   - What are two effective strategies that can be implemented at a community level to mitigate the effects of climate change?',\n",
       "  '',\n",
       "  '4. **Matching: Climate Change Effects**',\n",
       "  '   - Match the following effects of climate change (numbered) with their likely consequences (lettered).',\n",
       "  '     1. Rising sea levels',\n",
       "  '     2. Increased frequency of extreme weather events',\n",
       "  '     3. Melting polar ice caps',\n",
       "  '     4. Ocean acidification',\n",
       "  '     ',\n",
       "  '     A) Displacement of coastal communities',\n",
       "  '     B) Loss of marine biodiversity',\n",
       "  '     C) Increased global temperatures',\n",
       "  '     D) More frequent and severe hurricanes and floods',\n",
       "  '',\n",
       "  '5. **Essay: International Cooperation**',\n",
       "  '   - Discuss the importance of international cooperation in combating climate change. Include examples of successful global agreements or initiatives and explain how they have contributed to addressing climate change.'],\n",
       " 'results': ['```json\\n{\\n  \"Relevance\": 5,\\n  \"Completeness\": 4,\\n  \"Conciseness\": 3\\n}\\n```',\n",
       "  '```json\\n{\\n  \"Relevance\": 5,\\n  \"Completeness\": 4,\\n  \"Conciseness\": 3\\n}\\n```',\n",
       "  '```json\\n{\\n  \"Relevance\": 2,\\n  \"Completeness\": 1,\\n  \"Conciseness\": 2\\n}\\n```',\n",
       "  '```json\\n{\\n  \"Relevance\": 4,\\n  \"Completeness\": 3,\\n  \"Conciseness\": 3\\n}\\n```',\n",
       "  '```json\\n{\\n  \"Relevance\": 5,\\n  \"Completeness\": 4,\\n  \"Conciseness\": 3\\n}\\n```',\n",
       "  '```json\\n{\\n  \"Relevance\": 1,\\n  \"Completeness\": 1,\\n  \"Conciseness\": 2\\n}\\n```',\n",
       "  '```json\\n{\\n  \"Relevance\": 1,\\n  \"Completeness\": 1,\\n  \"Conciseness\": 2\\n}\\n```',\n",
       "  '```json\\n{\\n  \"Relevance\": 5,\\n  \"Completeness\": 4,\\n  \"Conciseness\": 3\\n}\\n```',\n",
       "  '```json\\n{\\n  \"Relevance\": 5,\\n  \"Completeness\": 4,\\n  \"Conciseness\": 3\\n}\\n```',\n",
       "  '```json\\n{\\n  \"Relevance\": 2,\\n  \"Completeness\": 1,\\n  \"Conciseness\": 2\\n}\\n```',\n",
       "  '```json\\n{\\n  \"Relevance\": 2,\\n  \"Completeness\": 1,\\n  \"Conciseness\": 2\\n}\\n```',\n",
       "  '```json\\n{\\n  \"Relevance\": 4,\\n  \"Completeness\": 3,\\n  \"Conciseness\": 2\\n}\\n```',\n",
       "  '```json\\n{\\n  \"Relevance\": 2,\\n  \"Completeness\": 1,\\n  \"Conciseness\": 2\\n}\\n```',\n",
       "  '```json\\n{\\n  \"Relevance\": 4,\\n  \"Completeness\": 3,\\n  \"Conciseness\": 3\\n}\\n```',\n",
       "  '```json\\n{\\n  \"Relevance\": 4,\\n  \"Completeness\": 2,\\n  \"Conciseness\": 3\\n}\\n```',\n",
       "  '```json\\n{\\n  \"Relevance\": 5,\\n  \"Completeness\": 4,\\n  \"Conciseness\": 3\\n}\\n```',\n",
       "  '```json\\n{\\n  \"Relevance\": 5,\\n  \"Completeness\": 4,\\n  \"Conciseness\": 3\\n}\\n```',\n",
       "  '```json\\n{\\n  \"Relevance\": 5,\\n  \"Completeness\": 4,\\n  \"Conciseness\": 3\\n}\\n```',\n",
       "  '```json\\n{\\n  \"Relevance\": 5,\\n  \"Completeness\": 4,\\n  \"Conciseness\": 3\\n}\\n```',\n",
       "  '```json\\n{\\n  \"Relevance\": 4,\\n  \"Completeness\": 3,\\n  \"Conciseness\": 3\\n}\\n```',\n",
       "  '```json\\n{\\n  \"Relevance\": 4,\\n  \"Completeness\": 3,\\n  \"Conciseness\": 2\\n}\\n```',\n",
       "  '```json\\n{\\n  \"Relevance\": 4,\\n  \"Completeness\": 3,\\n  \"Conciseness\": 3\\n}\\n```',\n",
       "  '```json\\n{\\n  \"Relevance\": 5,\\n  \"Completeness\": 4,\\n  \"Conciseness\": 3\\n}\\n```',\n",
       "  '```json\\n{\\n  \"Relevance\": 5,\\n  \"Completeness\": 4,\\n  \"Conciseness\": 3\\n}\\n```',\n",
       "  '```json\\n{\\n  \"Relevance\": 2,\\n  \"Completeness\": 1,\\n  \"Conciseness\": 2\\n}\\n```',\n",
       "  '```json\\n{\\n  \"Relevance\": 4,\\n  \"Completeness\": 3,\\n  \"Conciseness\": 3\\n}\\n```',\n",
       "  '```json\\n{\\n  \"Relevance\": 4,\\n  \"Completeness\": 2,\\n  \"Conciseness\": 3\\n}\\n```'],\n",
       " 'average_scores': None}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_rag(chunks_query_retriever)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
