{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/fusion_retrieval.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文档搜索中的融合检索\n",
    "\n",
    "## 概述\n",
    "\n",
    "此代码实现了一个融合检索系统，将基于向量的相似性搜索与基于关键词的BM25检索相结合。该方法旨在利用两种方法的优势来提高文档检索的整体质量和相关性。\n",
    "\n",
    "## 动机\n",
    "\n",
    "传统的检索方法通常依赖于语义理解（基于向量）或关键词匹配（BM25）。每种方法都有其优势和劣势。融合检索旨在结合这些方法，创建一个更强大和准确的检索系统，能够有效处理更广泛的查询。\n",
    "\n",
    "## 关键组件\n",
    "\n",
    "1. PDF处理和文本分块\n",
    "2. 使用FAISS和OpenAI嵌入创建向量存储\n",
    "3. 为基于关键词的检索创建BM25索引\n",
    "4. 结合两种方法的自定义融合检索函数\n",
    "\n",
    "## 方法详情\n",
    "\n",
    "### 文档预处理\n",
    "\n",
    "1. 使用RecursiveCharacterTextSplitter加载PDF并将其分割成块。\n",
    "2. 通过将't'替换为空格来清理块（可能解决特定的格式问题）。\n",
    "\n",
    "### 向量存储创建\n",
    "\n",
    "1. 使用OpenAI嵌入创建文本块的向量表示。\n",
    "2. 从这些嵌入创建FAISS向量存储以进行高效的相似性搜索。\n",
    "\n",
    "### BM25索引创建\n",
    "\n",
    "1. 从用于向量存储的相同文本块创建BM25索引。\n",
    "2. 这允许在基于向量的方法之外进行基于关键词的检索。\n",
    "\n",
    "### 融合检索函数\n",
    "\n",
    "`fusion_retrieval`函数是此实现的核心：\n",
    "\n",
    "1. 它接受查询并执行基于向量和基于BM25的检索。\n",
    "2. 将两种方法的分数标准化到共同的尺度。\n",
    "3. 计算这些分数的加权组合（由`alpha`参数控制）。\n",
    "4. 根据组合分数对文档进行排名，并返回前k个结果。\n",
    "\n",
    "## 此方法的优点\n",
    "\n",
    "1. 提高检索质量：通过结合语义和基于关键词的搜索，系统可以捕获概念相似性和精确关键词匹配。\n",
    "2. 灵活性：`alpha`参数允许根据特定用例或查询类型调整向量和关键词搜索之间的平衡。\n",
    "3. 鲁棒性：组合方法可以有效处理更广泛的查询，减轻单个方法的弱点。\n",
    "4. 可定制性：系统可以轻松适应使用不同的向量存储或基于关键词的检索方法。\n",
    "\n",
    "## 结论\n",
    "\n",
    "融合检索代表了一种强大的文档搜索方法，结合了语义理解和关键词匹配的优势。通过利用基于向量和BM25检索方法，它为信息检索任务提供了更全面和灵活的解决方案。这种方法在概念相似性和关键词相关性都很重要的各个领域都有潜在应用，如学术研究、法律文档搜索或通用搜索引擎。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "\n",
    "<img src=\"../images/fusion_retrieval.svg\" alt=\"Fusion Retrieval\" style=\"width:100%; height:auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 包安装和导入\n",
    "\n",
    "下面的单元格安装运行此notebook所需的所有必要包。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装所需包\n",
    "!pip install langchain numpy python-dotenv rank-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 克隆仓库以访问辅助函数和评估模块\n",
    "!git clone https://github.com/NirDiamant/RAG_TECHNIQUES.git\n",
    "import sys\n",
    "sys.path.append('RAG_TECHNIQUES')\n",
    "# 如果需要使用最新数据运行\n",
    "# !cp -r RAG_TECHNIQUES/data ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "from typing import List\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 为Colab兼容性替换原始路径追加\n",
    "from helper_functions import *\n",
    "from evaluation.evalute_rag import *\n",
    "\n",
    "# 从.env文件加载环境变量\n",
    "load_dotenv()\n",
    "\n",
    "# 设置OpenAI API密钥环境变量\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义文档路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载所需数据文件\n",
    "import os\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# 下载此notebook中使用的PDF文档\n",
    "!wget -O data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n",
    "!wget -O data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/Understanding_Climate_Change.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用OpenAI嵌入将PDF书籍编码到向量存储中。\n",
    "\n",
    "    Args:\n",
    "        path: PDF文件的路径。\n",
    "        chunk_size: 每个文本块的期望大小。\n",
    "        chunk_overlap: 连续块之间的重叠量。\n",
    "\n",
    "    Returns:\n",
    "        包含编码书籍内容的FAISS向量存储。\n",
    "    \"\"\"\n",
    "\n",
    "    # 加载PDF文档\n",
    "    loader = PyPDFLoader(path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # 将文档分割成块\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
    "    )\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    cleaned_texts = replace_t_with_space(texts)\n",
    "\n",
    "    # 创建嵌入和向量存储\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.from_documents(cleaned_texts, embeddings)\n",
    "\n",
    "    return vectorstore, cleaned_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建向量存储并获取分块文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore, cleaned_texts = encode_pdf_and_get_split_documents(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a bm25 index for retrieving documents by keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bm25_index(documents: List[Document]) -> BM25Okapi:\n",
    "    \"\"\"\n",
    "    从给定文档创建BM25索引。\n",
    "\n",
    "    BM25（最佳匹配25）是信息检索中使用的排名函数。\n",
    "    它基于概率检索框架，是对TF-IDF的改进。\n",
    "\n",
    "    Args:\n",
    "    documents (List[Document]): 要索引的文档列表。\n",
    "\n",
    "    Returns:\n",
    "    BM25Okapi: 可用于BM25评分的索引。\n",
    "    \"\"\"\n",
    "    # 通过空白字符分割对每个文档进行分词\n",
    "    # 这是一种简单的方法，可以通过更复杂的分词来改进\n",
    "    tokenized_docs = [doc.page_content.split() for doc in documents]\n",
    "    return BM25Okapi(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = create_bm25_index(cleaned_texts) # 从清理后的文本（块）创建BM25索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function that retrieves both semantically and by keyword, normalizes the scores and gets the top k documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fusion_retrieval(vectorstore, bm25, query: str, k: int = 5, alpha: float = 0.5) -> List[Document]:\n",
    "    \"\"\"\n",
    "    执行融合检索，结合基于关键词（BM25）和基于向量的搜索。\n",
    "\n",
    "    Args:\n",
    "    vectorstore (VectorStore): 包含文档的向量存储。\n",
    "    bm25 (BM25Okapi): 预计算的BM25索引。\n",
    "    query (str): 查询字符串。\n",
    "    k (int): 要检索的文档数量。\n",
    "    alpha (float): 向量搜索分数的权重（1-alpha将是BM25分数的权重）。\n",
    "\n",
    "    Returns:\n",
    "    List[Document]: 基于组合分数的前k个文档。\n",
    "    \"\"\"\n",
    "    \n",
    "    epsilon = 1e-8\n",
    "\n",
    "    # 步骤1：从向量存储获取所有文档\n",
    "    all_docs = vectorstore.similarity_search(\"\", k=vectorstore.index.ntotal)\n",
    "\n",
    "    # 步骤2：执行BM25搜索\n",
    "    bm25_scores = bm25.get_scores(query.split())\n",
    "\n",
    "    # 步骤3：执行向量搜索\n",
    "    vector_results = vectorstore.similarity_search_with_score(query, k=len(all_docs))\n",
    "    \n",
    "    # 步骤4：标准化分数\n",
    "    vector_scores = np.array([score for _, score in vector_results])\n",
    "    vector_scores = 1 - (vector_scores - np.min(vector_scores)) / (np.max(vector_scores) - np.min(vector_scores) + epsilon)\n",
    "\n",
    "    bm25_scores = (bm25_scores - np.min(bm25_scores)) / (np.max(bm25_scores) -  np.min(bm25_scores) + epsilon)\n",
    "\n",
    "    # 步骤5：组合分数\n",
    "    combined_scores = alpha * vector_scores + (1 - alpha) * bm25_scores  \n",
    "\n",
    "    # 步骤6：对文档进行排名\n",
    "    sorted_indices = np.argsort(combined_scores)[::-1]\n",
    "    \n",
    "    # 步骤7：返回前k个文档\n",
    "    return [all_docs[i] for i in sorted_indices[:k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用例示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查询\n",
    "query = \"What are the impacts of climate change on the environment?\"\n",
    "\n",
    "# 执行融合检索\n",
    "top_docs = fusion_retrieval(vectorstore, bm25, query, k=5, alpha=0.5)\n",
    "docs_content = [doc.page_content for doc in top_docs]\n",
    "show_context(docs_content)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
